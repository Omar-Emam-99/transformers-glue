{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom bertviz.transformers_neuron_view import BertModel\\nfrom bertviz.neuron_view import show\\nfrom bertviz import head_view\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer , AutoModel\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig\n",
    "import math , warnings , os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "\"\"\"\n",
    "from bertviz.transformers_neuron_view import BertModel\n",
    "from bertviz.neuron_view import show\n",
    "from bertviz import head_view\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct BERT Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a scaled dot product function for (q,k,v) for self-attention\n",
    "so we get a sentance of len and embedded it so we get a [batch_size  , seq_len , hidden_state] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_porduct_attention(query,key,value):\n",
    "  \n",
    "  dim_k = key.size(-1)\n",
    "  #batch matrix-matrix product to ignore batch dims\n",
    "  scores = torch.bmm(query , key.transpose(1,2)) / sqrt(dim_k)\n",
    "  #apply softmax \n",
    "  weights = F.softmax(scores , dim=-1)\n",
    "  return torch.bmm(weights , value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_porduct_attention(query,key,value):\n",
    "  \n",
    "  dim_k = key.size(-1)\n",
    "  #batch matrix-matrix product to ignore batch dims\n",
    "  scores = torch.bmm(query , key.transpose(1,2)) / sqrt(dim_k)\n",
    "  #apply softmax \n",
    "  weights = F.softmax(scores , dim=-1)\n",
    "  return torch.bmm(weights , value)\n",
    "# one head self-attention\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim:int, head_dim:int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim , head_dim)\n",
    "        self.k = nn.Linear(embed_dim , head_dim)\n",
    "        self.v = nn.Linear(embed_dim , head_dim)\n",
    "    def forward(self , hidden_state):\n",
    "        #compute self-attention\n",
    "        atten_outputs = scaled_dot_porduct_attention(self.q(hidden_state) ,\n",
    "                                                     self.k(hidden_state),\n",
    "                                                     self.v(hidden_state))\n",
    "        return atten_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-headed attention \n",
    "\"\"\"\n",
    "with one head self-attention :\n",
    "    the softmax of one head tends to focus on mostly one aspect of similarity.\n",
    "    Having several heads allows the model to focus on several aspects at once .\n",
    "\n",
    "\"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self ,num_attention_heads:int,hidden_size:int):\n",
    "        super().__init__()\n",
    "        self.embedd_dim =  hidden_size #768\n",
    "        self.num_heads = num_attention_heads #12 \n",
    "        self.head_dim = self.embedd_dim // self.num_heads #768//12 = 64\n",
    "        self.heads = nn.ModuleList(\n",
    "            AttentionHead(self.embedd_dim , self.head_dim) for _ in range(self.num_heads) \n",
    "        )\n",
    "        self.outputs = nn.Linear(self.embedd_dim , self.embedd_dim)\n",
    "    \n",
    "    def forward(self , hidden_state ):\n",
    "        #concate All 12 head of 64 dim to 768 dim\n",
    "        concate_heads = torch.cat([head(hidden_state) for head in self.heads] , dim=-1)\n",
    "        #pass the concate_heads into a Linear layer \n",
    "        multi_heads_outputs = self.outputs(concate_heads)\n",
    "        return multi_heads_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#position-wise feed-forward layer\n",
    "\"\"\"\n",
    "instead of processing the whole sequence of embeddings as a single vector,\n",
    "it processes each embedding independently Like (one-dimensional convolution with a kernel size) .\n",
    "\"\"\"\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self , hidden_size:int,\n",
    "                 intermidiate_size:int,\n",
    "                 hidden_dropout_prob:int):\n",
    "        super().__init__()\n",
    "        #four times the size of the embeddings\n",
    "        self.linear_layer_1 = nn.Linear(hidden_size ,intermidiate_size)\n",
    "        self.linear_layer_2 = nn.Linear(intermidiate_size , hidden_size)\n",
    "        self.glue = nn.GELU()\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "    def forward(self , x):\n",
    "        x = self.linear_layer_1(x)\n",
    "        x = self.linear_layer_2(x)\n",
    "        x = self.glue(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normaliztion\n",
    "\"\"\"\n",
    "The former normalizes each input in the batch to have zero mean and unity variance.\n",
    "two approaches :\n",
    "    - Post layer normalization \n",
    "    - Pre layer normalization\n",
    "\"\"\"\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self ,hidden_size,num_attention_heads, intermidiate_size):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.attention = MultiHeadAttention(num_attention_heads , hidden_size)\n",
    "        self.feedforward = FeedForward(hidden_size , intermidiate_size)\n",
    "    def forward(self , x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feedforward(self.layer_norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "class BertConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`BertModel`] or a [`TFBertModel`]. It is used to\n",
    "    instantiate a BERT model according to the specified arguments, defining the model architecture. Instantiating a\n",
    "    configuration with the defaults will yield a similar configuration to that of the BERT\n",
    "    [bert-base-uncased](https://huggingface.co/bert-base-uncased) architecture.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        vocab_size (`int`, *optional*, defaults to 30522):\n",
    "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n",
    "            `inputs_ids` passed when calling [`BertModel`] or [`TFBertModel`].\n",
    "        hidden_size (`int`, *optional*, defaults to 768):\n",
    "            Dimensionality of the encoder layers and the pooler layer.\n",
    "        num_hidden_layers (`int`, *optional*, defaults to 12):\n",
    "            Number of hidden layers in the Transformer encoder.\n",
    "        num_attention_heads (`int`, *optional*, defaults to 12):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        intermediate_size (`int`, *optional*, defaults to 3072):\n",
    "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
    "        hidden_act (`str` or `Callable`, *optional*, defaults to `\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n",
    "            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n",
    "        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "        max_position_embeddings (`int`, *optional*, defaults to 512):\n",
    "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
    "            just in case (e.g., 512 or 1024 or 2048).\n",
    "        type_vocab_size (`int`, *optional*, defaults to 2):\n",
    "            The vocabulary size of the `token_type_ids` passed when calling [`BertModel`] or [`TFBertModel`].\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n",
    "            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n",
    "            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n",
    "            [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).\n",
    "            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n",
    "            with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).\n",
    "        is_decoder (`bool`, *optional*, defaults to `False`):\n",
    "            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n",
    "        use_cache (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if `config.is_decoder=True`.\n",
    "        classifier_dropout (`float`, *optional*):\n",
    "            The dropout ratio for the classification head.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import BertConfig, BertModel\n",
    "\n",
    "    >>> # Initializing a BERT bert-base-uncased style configuration\n",
    "    >>> configuration = BertConfig()\n",
    "\n",
    "    >>> # Initializing a model (with random weights) from the bert-base-uncased style configuration\n",
    "    >>> model = BertModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"bert\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        pad_token_id=0,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        use_cache=True,\n",
    "        classifier_dropout=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        self.use_cache = use_cache\n",
    "        self.classifier_dropout = classifier_dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model For Text classification\n",
    "> BERT using MLM and NSP \n",
    "### BERT EmbeddingLayer :\n",
    "    consist of three parts\n",
    "*   **word-embedding** :\n",
    "        BERT uses an initial token embedding layer to convert input tokens into dense vectors of fixed dimensionality , so its feed into the model and it learned during training allow the model to capture \n",
    "        meaning of individual words (USING Word-piece).\n",
    "*   **segment-embedding** :\n",
    "        Segment embeddings are basically the sentence number that is encoded into a vector.\n",
    "*   **positional-embedding** :\n",
    "        Position embeddings are the position of the word within that sentence that is encoded into a vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type(segment-Embedding) embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        #segment-embeddings\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        #registered as part of a module's state\n",
    "        \n",
    "        #They are typically used to store non-learnable state information that is associated with\n",
    "        #  a module, such as running statistics for batch normalization or position embeddings in\n",
    "        #    a transformer\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)),\n",
    "            persistent=False\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long),\n",
    "            persistent=False\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values_length: int = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
    "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
    "        # issue #5664\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "#Create Class for one_head_attention\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self , embd_dim , head_dim):\n",
    "    super().__init__()\n",
    "    self.Q = nn.Linear(embd_dim , head_dim)\n",
    "    self.K = nn.Linear(embd_dim , head_dim)\n",
    "    self.V = nn.Linear(embd_dim , head_dim)\n",
    "  def scaled_dot_porduct_attention(query , key , value , masked=None):\n",
    "    dim_k = key.size(-1)\n",
    "    scores = torch.bmm(query , key.transpose(1,2)) / sqrt(dim_k)\n",
    "    if masked is not None :\n",
    "      scores = scores.masked_fill(mask==0 , float(\"inf\"))\n",
    "    weights = F.softmax(scores , dim=-1)\n",
    "    return torch.bmm(weights , value)\n",
    "  def forward(self , hidden_state):\n",
    "    attn_outs = scaled_dot_porduct_attention(self.Q(hidden_state) ,\n",
    "                                             self.K(hidden_state) , self.V(hidden_state))\n",
    "    return attn_outs\n",
    "  \n",
    "\n",
    "#create a MultiHeaded Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self , config):\n",
    "    super().__init__()\n",
    "    embd_dim = config.hidden_size #768\n",
    "    num_heads = config.num_attention_heads #12\n",
    "    head_dim = embd_dim // num_heads #64\n",
    "    self.heads = nn.ModuleList(\n",
    "        AttentionHead(embd_dim , head_dim) for _ in range(num_heads)\n",
    "    )\n",
    "    self.output_layer = nn.Linear(embd_dim , embd_dim) #input 768 dim --> get a 768 dim represent a Attention Multi head\n",
    "\n",
    "  def forward(self , hidden_state):\n",
    "    #concate All 12 head of 64 dim to 768 dim\n",
    "    x = torch.cat([h(hidden_state) for h in self.heads] , dim=-1)\n",
    "    #push them into a Linear layer\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFeedForward(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.Linear_Layer_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "    self.Linear_Layer_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    self.gelu = nn.GELU()\n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "  def forward(self, hidden_state):\n",
    "    hidden_state = self.Linear_Layer_1(hidden_state)\n",
    "    hidden_state = self.gelu(hidden_state)\n",
    "    hidden_state = self.Linear_Layer_2(hidden_state)\n",
    "    hidden_state = self.dropout(hidden_state)\n",
    "    return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncodingLayer(nn.Module):\n",
    "  def __init__(self , config):\n",
    "    super().__init__()\n",
    "    self.LayerNorm_1 = nn.LayerNorm(config.hidden_size)\n",
    "    self.LayerNorm_2 = nn.LayerNorm(config.hidden_size)\n",
    "    self.attention = MultiHeadAttention(config)\n",
    "    self.feed_forward = FeedForward(config)\n",
    "\n",
    "  def forward(self , x):\n",
    "    #apply layer normaliztion and then copy input into Q ,K and V\n",
    "    hidden_state = self.LayerNorm_1(x)\n",
    "    #apply Attention with skip connection\n",
    "    x = x + self.attention(hidden_state)\n",
    "    #apply FF layer with skip connection\n",
    "    x = x + self.feed_forward(self.LayerNorm_2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.Embeddings = BertEmbeddings(config)\n",
    "        \n",
    "        self.BertLayers = nn.ModuleList([BertEncodingLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "    def forward(self , input_tokens):\n",
    "        \n",
    "        input_tokens = self.Embeddings(input_tokens)\n",
    "        for bertlayer in self.BertLayers :\n",
    "            input_tokens = bertlayer(input_tokens)\n",
    "        return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1045,  2572, 13192,  1998,  1045,  2293,  2374,   102]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'size'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a6f6c22f6cbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-0b9638752215>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tokens)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0minput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbertlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertLayers\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0minput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-ea255c9539af>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ckp = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "text = \"hello i am omar and i love football\"\n",
    "encode = tokenizer(text , return_tensors='pt')\n",
    "#tokens = tokenizer.convert_ids_to_tokens(encode.input_ids)\n",
    "print(encode['input_ids'])\n",
    "####------------------------------ Test Embedding Class-------------\n",
    "\n",
    "\n",
    "config = BertConfig()\n",
    "\"\"\"\n",
    "embeddings = BertEmbeddings(config)\n",
    "embed = embeddings(encode.input_ids)\n",
    "print(embed.size())\n",
    "\n",
    "atten = MultiHeadAttention(config)\n",
    "outs = atten(embed)\n",
    "\n",
    "feedouts = BertFeedForward(config)\n",
    "outsoffeed = feedouts(outs)\n",
    "print(outsoffeed.size())\n",
    "\"\"\"\n",
    "\n",
    "bert = BertEncoder(config)\n",
    "\n",
    "outputs = bert(encode)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads) \n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        \n",
    "        #relative_key is relative distance between two tokens in a sequence of text ,\n",
    "        #like in machine translation\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "        \n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        \n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        use_cache = past_key_value is not None\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        \n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
    "            if use_cache:\n",
    "                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n",
    "                    -1, 1\n",
    "                )\n",
    "            else:\n",
    "                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        \n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        \n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMultiHeadAttention(nn.Module):\n",
    "    def __init__(self , config):\n",
    "        super().__init__()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class TransfomersEncodingLayer(nn.Module):\n",
    "  def __init__(self , config):\n",
    "    super().__init__()\n",
    "    self.layer_norm1 = nn.LayerNorm(config.hidden_size)\n",
    "    self.layer_norm2 = nn.LayerNorm(config.hidden_size)\n",
    "    self.attention = BertSelfAttention(config)\n",
    "    self.feed_forward = FeedForward(config)\n",
    "\n",
    "  def forward(self , x):\n",
    "    #apply layer normaliztion and then copy input into Q ,K and V\n",
    "    hidden_state = self.layer_norm1(x)\n",
    "    #apply Attention with skip connection\n",
    "    outs = self.attention(hidden_state)\n",
    "    x = x + outs[0]\n",
    "    #apply FF layer with skip connection\n",
    "    x = x + self.feed_forward(self.layer_norm2(x))\n",
    "    return x\n",
    "\n",
    "class TransfomersEncoder(nn.Module):\n",
    "  def __init__(self , config):\n",
    "    super().__init__()\n",
    "    self.embeddings = BertEmbeddings(config)\n",
    "    self.layers = nn.ModuleList([TransfomersEncodingLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "  def forward(self , x):\n",
    "    x = self.embeddings(x)\n",
    "    for layer in self.layers :\n",
    "      x = layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1045,  2572, 13192,  1998,  1045,  2293,  2374,   102]])\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "model_ckp = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "text = \"hello i am omar and i love football\"\n",
    "encode = tokenizer(text , return_tensors='pt')\n",
    "#tokens = tokenizer.convert_ids_to_tokens(encode.input_ids)\n",
    "print(encode.input_ids)\n",
    "####------------------------------ Test Embedding Class-------------\n",
    "config = BertConfig()\n",
    "embeddings = BertEmbeddings(config)\n",
    "embed = embeddings(encode.input_ids)\n",
    "print(embed.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attetion = BertSelfAttention(config)\n",
    "outs = attetion(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outs[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT = TransfomersEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = BERT(encode.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "  def __init__(self ,config):\n",
    "    super().__init__()\n",
    "    self.encoder = TransfomersEncoder(config)\n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    self.classifier = nn.Linear(config.hidden_size , config.num_labels)\n",
    "  def forward(self , X):\n",
    "    hidden = self.encoder(X)[: , 0 , :]\n",
    "    x = self.dropout(hidden)\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerForSequenceClassification(\n",
       "  (encoder): TransfomersEncoder(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): TransfomersEncodingLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels = 3\n",
    "encoder_classifier = TransformerForSequenceClassification(config)\n",
    "encoder_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchsummary import  summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Create Class for one_head_attention\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self , embd_dim , head_dim):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.Query = nn.Linear(embd_dim , head_dim)\n",
    "    self.Key   = nn.Linear(embd_dim , head_dim)\n",
    "    self.Value = nn.Linear(embd_dim , head_dim)\n",
    "\n",
    "  def forward(self , hidden_state):\n",
    "\n",
    "    Query , key , value = self.Query(hidden_state) , self.Key(hidden_state) , self.Value(hidden_state)\n",
    "    dim_k = key.size(-1)\n",
    "    scores = torch.bmm(Query , key.transpose(1,2)) / sqrt(dim_k)\n",
    "    weights = F.softmax(scores , dim=-1)\n",
    "    attn_outs = torch.bmm(weights , value)\n",
    "\n",
    "    return attn_outs\n",
    "  \n",
    "\n",
    "#create a MultiHeaded Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self , config):\n",
    "    super().__init__()\n",
    "    embd_dim = config.hidden_size #768\n",
    "    num_heads = config.num_attention_heads #12\n",
    "    head_dim = embd_dim // num_heads #64\n",
    "    self.heads = nn.ModuleList(\n",
    "        AttentionHead(embd_dim , head_dim) for _ in range(num_heads)\n",
    "    )\n",
    "    self.output_layer = nn.Linear(embd_dim , embd_dim) #input 768 dim --> get a 768 dim represent a Attention Multi head\n",
    "\n",
    "  def forward(self , hidden_state):\n",
    "    #concate All 12 head of 64 dim to 768 dim\n",
    "    x = torch.cat([h(hidden_state) for h in self.heads] , dim=-1)\n",
    "    #push them into a Linear layer\n",
    "    x = self.output_layer(x)\n",
    "    return x\n",
    "class BertFeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, hidden_state):\n",
    "        hidden_state = self.linear_1(hidden_state)\n",
    "        hidden_state = self.gelu(hidden_state)\n",
    "        hidden_state = self.linear_2(hidden_state)\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        return hidden_state\n",
    "\n",
    "class BertEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = BertFeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.layers = nn.ModuleList([BertEncoderLayer(config) \n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1045,  2572, 13192,  1998,  1045,  2293,  2374,   102]])\n"
     ]
    }
   ],
   "source": [
    "model_ckp = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "text = \"hello i am omar and i love football\"\n",
    "encode = tokenizer(text , return_tensors='pt')\n",
    "#tokens = tokenizer.convert_ids_to_tokens(encode.input_ids)\n",
    "print(encode['input_ids'])\n",
    "####------------------------------ Test Embedding Class-------------\n",
    "\n",
    "\n",
    "config = BertConfig()\n",
    "\"\"\"\n",
    "embeddings = BertEmbeddings(config)\n",
    "embed = embeddings(encode.input_ids)\n",
    "print(embed.size())\n",
    "\n",
    "atten = MultiHeadAttention(config)\n",
    "outs = atten(embed)\n",
    "\n",
    "feedouts = BertFeedForward(config)\n",
    "outsoffeed = feedouts(outs)\n",
    "print(outsoffeed.size())\n",
    "\"\"\"\n",
    "\n",
    "bert = BertEncoder(config)\n",
    "\n",
    "outputs = bert(encode.input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
