{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom bertviz.transformers_neuron_view import BertModel\\nfrom bertviz.neuron_view import show\\nfrom bertviz import head_view\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer , AutoModel\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig\n",
    "from math import sqrt\n",
    "import  warnings , os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "\"\"\"\n",
    "from bertviz.transformers_neuron_view import BertModel\n",
    "from bertviz.neuron_view import show\n",
    "from bertviz import head_view\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig():\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`BertModel`] or a [`TFBertModel`]. It is used to\n",
    "    instantiate a BERT model according to the specified arguments, defining the model architecture. Instantiating a\n",
    "    configuration with the defaults will yield a similar configuration to that of the BERT\n",
    "    [bert-base-uncased](https://huggingface.co/bert-base-uncased) architecture.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        vocab_size (`int`, *optional*, defaults to 30522):\n",
    "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n",
    "            `inputs_ids` passed when calling [`BertModel`] or [`TFBertModel`].\n",
    "        hidden_size (`int`, *optional*, defaults to 768):\n",
    "            Dimensionality of the encoder layers and the pooler layer.\n",
    "        num_hidden_layers (`int`, *optional*, defaults to 12):\n",
    "            Number of hidden layers in the Transformer encoder.\n",
    "        num_attention_heads (`int`, *optional*, defaults to 12):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        intermediate_size (`int`, *optional*, defaults to 3072):\n",
    "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
    "        hidden_act (`str` or `Callable`, *optional*, defaults to `\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n",
    "            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n",
    "        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "        max_position_embeddings (`int`, *optional*, defaults to 512):\n",
    "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
    "            just in case (e.g., 512 or 1024 or 2048).\n",
    "        type_vocab_size (`int`, *optional*, defaults to 2):\n",
    "            The vocabulary size of the `token_type_ids` passed when calling [`BertModel`] or [`TFBertModel`].\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n",
    "            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n",
    "            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n",
    "            [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).\n",
    "            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n",
    "            with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).\n",
    "        is_decoder (`bool`, *optional*, defaults to `False`):\n",
    "            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n",
    "        use_cache (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if `config.is_decoder=True`.\n",
    "        classifier_dropout (`float`, *optional*):\n",
    "            The dropout ratio for the classification head.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import BertConfig, BertModel\n",
    "\n",
    "    >>> # Initializing a BERT bert-base-uncased style configuration\n",
    "    >>> configuration = BertConfig()\n",
    "\n",
    "    >>> # Initializing a model (with random weights) from the bert-base-uncased style configuration\n",
    "    >>> model = BertModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"bert\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        pad_token_id=0,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        use_cache=True,\n",
    "        classifier_dropout=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        self.use_cache = use_cache\n",
    "        self.classifier_dropout = classifier_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type(segment-Embedding) embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        #segment-embeddings\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        #registered as part of a module's state\n",
    "        \n",
    "        #They are typically used to store non-learnable state information that is associated with\n",
    "        #  a module, such as running statistics for batch normalization or position embeddings in\n",
    "        #    a transformer\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)),\n",
    "            persistent=False\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long),\n",
    "            persistent=False\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values_length: int = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
    "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
    "        # issue #5664\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1045,  2572, 13192,  1998,  1045,  2293,  2374,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "model_ckp = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "text = \"hello i am omar and i love football\"\n",
    "encode = tokenizer(text , return_tensors='pt')\n",
    "print(encode)\n",
    "print(encode.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig()\n",
    "embeddings = BertEmbeddings(config)\n",
    "embedds = embeddings(encode.input_ids)\n",
    "embedds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.Query = nn.Linear(embed_dim, head_dim)\n",
    "        self.Key = nn.Linear(embed_dim, head_dim)\n",
    "        self.Value = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        Query , Key , Value = self.Query(hidden_state) , self.Key(hidden_state) , self.Value(hidden_state)\n",
    "        dim_k = Query.size(-1)\n",
    "        scores = torch.bmm(Query, Key.transpose(1, 2)) / sqrt(dim_k)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        atten_output = torch.bmm(weights, Value)\n",
    "        return atten_output\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = MultiHeadAttention(config)\n",
    "outs = atten(embedds)\n",
    "outs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward = BertFeedForward(config)\n",
    "ff_outputs = feed_forward(outs)\n",
    "ff_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = BertFeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 768]), torch.Size([1, 10, 768]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = BertEncoderLayer(config)\n",
    "embedds.shape, encoder_layer(embedds).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.layers = nn.ModuleList([BertEncoderLayer(config) \n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerForSequenceClassification(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertEncoderLayer(\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (heads): ModuleList(\n",
       "            (0): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (1): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (2): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (3): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (4): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (5): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (6): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (7): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (8): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (9): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (10): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "            (11): AttentionHead(\n",
       "              (Query): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Key): Linear(in_features=768, out_features=64, bias=True)\n",
       "              (Value): Linear(in_features=768, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (feed_forward): BertFeedForward(\n",
       "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (gelu): GELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = TransformerForSequenceClassification(config)\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
